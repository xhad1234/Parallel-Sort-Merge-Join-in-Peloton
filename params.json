{
  "name": "Parallel Sort Merge Join in Peloton",
  "tagline": "The Self-Driving Database Management System",
  "body": "Currently we are implementing the SIMD sort. Because it is independent from peloton, we put the current code in a different repository: [avx2-merge-sort](https://github.com/sid1607/avx2-merge-sort)\r\n***\r\n# Proposal\r\n\r\n## Summary\r\n\r\nThe objective is to implement the Sort-Merge Join operator into the Peloton in-memory database system made by [CMUDB](https://github.com/cmu-db/peloton). Recent database literature have designed various hardware-aware Sort-Merge join approaches that involve cache-conscious sorting mechanisms and NUMA-aware work decomposition schemes. We will incorporate these designs in the operator we build for Peloton and evaluate the performance gains and hardware utilization that these techniques have to offer.\r\n\r\n## Background\r\nPeloton is an in-memory database project that is developed by the CMU Database Group based on the RDBMS model with ACID guarantees. Since this system does not use the disk for storing and retrieving data, it needs to address bottlenecks associated with the memory hierarchy (caches), memory access patterns, concurrency control across multiple cores in order to utilize the available hardware effectively. Peloton, being a HTAP (Hybrid Transaction Processing) system, should be capable of handling both OLTP and OLAP workloads. OLAP involves long running queries that operate on large volumes of data across multiple relations. Joins are in important class of operators for OLAP workloads and OLAP joins have good scope for parallelization since they work on large volumes of data in memory. This approach to database parallelism is known as > intra-query parallelism. Presently, Peloton does not support intra-query parallelism for any class of queries. Based on the relevance to what we learnt from this course and the approaches followed by existing database literature, we chose to work with sort-merge join.\r\n\r\nA join algorithm accepts two input relations and merges corresponding tuples that match the predicate applied on the join attributes of the two relations. The classical Sort-Merge join algorithm consists of 3 phases - \r\n\r\n* An optional partitioning phase - this phase is used to partition the input relations, which could be in the form of dividing the input load across different workers or cores.\r\n\r\n* The sort phase - this phase sorts the tuples of input relations based on the join key\r\n\r\n* The merge phase - this step scans through the sorted runs of the input relations in lock-step and merges a pair of tuples if the join predicate is satisfied. Assuming no duplicate tuples in either relation, merge can be performed in a single pass.\r\n\r\nThe biggest challenger for parallelization is the sort phase since there are a lot of data dependencies in all sorting algorithms. The work done in Oracle and Intel's \"Fast Join Implementation on Modern Multi-core CPUS\" and  ETH's \"new optimization and results for radix hash join\" use a cache-conscious approach for the sort phase. These approaches implement merge sort and apply a different implementation for merge sort depending on the level in the memory hierarchy. For register-level and cache-level sorting, these papers claim that using SIMD is important to achieve good speedup and hardware efficiency. The work done in Hyper claims that SIMD-based sorting isn't necessary to achieve good performance and processor scalability for sort-merge join. They propose a NUMA-aware implementation that incurs the partitioning phase to chunk the input relations across NUMA-sockets, produces core-local sorted runs of these chunks and applies the join predicate for each chunk of the first relation against all chunks of the second relation. The rationale for this approach is that complete traversal of the second relation's chunks will not happen for any given first chunk, since they are already sorted. They also state that the memory prefetcher can mask the interconnect latency when sequential access is carried out on non-local sockets. Our project would be focussed towards integrating the key ideas introduced by these approaches - using a SIMD-based low-level sorting implementation and NUMA-aware data partitioning and evaluating the performance and scalability of our implementation as we add these features\r\n\r\n## The Challenge\r\nWe will be using AVX2 CPU intrinsics to implement cache-conscious sorting with SIMD. Since AVX2 offers low-level primitives it will be fairly complicated to implement a fully-functioning sorting network. We also need to be careful about the type of AVX2 instructions we use; an implementation with unnecessary calls to gather and scatter can limit the performance of the implementation. The workload involves two large relations that need to be sorted, which inherently comes with a high communication-to-computation ratio, since large tables will certainly not fit a single memory region. Since sorting is an operation that attempts to establish global order across distributed elements, reducing and optimization the communication patterns would be the primary challenge. None of the papers discussed in the previous section have a perfect solution for this. Instead, they rely on data access patterns and instructional similarity in some operations of join to make better utilization of the hardware.\r\n\r\n## Resource\r\nWe are working based on the code base of  the CMU in-memory DBMS project Peloton. For SIMD merge-sort method, we plan to refer to the oracle and intel's sort merge work \r\n\r\n`C. Kim, T. Kaldewey, V. W. Lee, E. Sedlar, A. D. Nguyen, N. Satish, J. Chhugani, A. Di Blas,and P. Dubey.  Sort vs. hash revisited:  fast join implementation on modern multi-core cpus.Proceedings of the VLDB Endowment, 2(2):1378–1389, 2009.`\r\n\r\nFor the numa-aware improvement, we plan to refer to the method provided by the Hyper work\r\n\r\n`M.-C.  Albutiu,  A.  Kemper,  and  T.  Neumann.   Massively  parallel  sort-merge  joins  in  mainmemory multi-core database systems.Proceedings of the VLDB Endowment, 5(10):1064–1075,201.`\r\n\r\nFor the partitioning improvement, we plan to refer to the Radix Hash Join method\r\n\r\n`C.  Balkesen,  J.  Teubner,  G.  Alonso,  and  M.  T. ̈Ozsu.   Main-memory  hash  joins  on  multi-core cpus:  Tuning to the underlying hardware.  InData Engineering (ICDE), 2013 IEEE 29thInternational Conference on, pages 362–373. IEEE, 2013.`\r\n\r\nThe parallel data lab provides PDL clusters for us to do the evaluation. There are nodes of NUMA architecture inside so as we can test the numa-aware implementation based on that.  Also, we plan to use TPC-h benchmark to compare the scalabilities among the non-paralel , non-NUMA-aware SIMD and NUMA-aware versions of peloton.\r\n\r\n## Goal and Deliverables\r\n\r\n* Minimum Goal: We set the minimum goal as the implementation of the logistic of the SIMD version merge-sort, together with a thorough performance analysis for that. We don't expect scalability for the minimum goal. The deliverable for this goal includes a consistent and runnable SIMD merge-join mechanism for peloton, and a report comparing and analysing of the performance among the sequential and parallel version. We expect a B score for reaching minimum goal. \r\n\r\n* Main Goal: The main goal is an scalable parallel merge-sort join mechanism of Peloton, together with the evaluation and performance analysis. Compared to minimum goal, we want to further optimize the method so as the performance of sort-merge join could scale when number of cores increase. The deliverable for this goal includes the scalable sort-merge join mechanism based on SIMD and an through evaluation report of that. We expect an A scrore for reaching main goal.\r\n\r\n* Stretch Goal: We have two stretch goals. The first stretch goal is to implement a NUMA-aware merge-sort join mechanism together with the evaluation showing the benefit gained from that. The second stretch goal is to implement the radix hash join partitioning method to get further performance improvement. The deliverable for this phase is the implementation together with an evaluation based on machine with NUMA architecture. We expect an A+ for accomplish either of the stretch goals.\r\n\r\n\r\n## Platform Choice\r\nWe use PDL cluster as the evaluation machine because it provides NUMA node. We choose C++ as the programming language since it has SIMD lib and it is the language of Peloton.\r\n\r\n***\r\n\r\n# Project Checkpoint\r\n\r\n## Revised Schedule\r\n\r\nWeek Number | Tasks\r\n----------- | -----------------\r\n1   | ✔️ Implement a sequential merge-sort join mechanism for Peloton\r\n2   | ✔️ Design and implement the SIMD version merge-sort\r\n3   | ✔️ Finish the SIMD version merge-sort \r\n    | ✔️ Testing the SIMD version merge-sort\r\n4-1 | (Sid) Support sorting of all elements number  \r\n    | (Lei) Micro benchmark on Haswell Machine to compare std vs SIMD sort  \r\n4-2 | (Sid) Explore 16*16 bitonic merge\r\n    | (Lei) Compare with 4-fold network\r\n5-1 | (Sid) AVX2 compile with Peloton\r\n    | (Lei) Merge the SIMD sort in Peloton\r\n5-2 | (Sid & Lei) Figure out how to maintain the reference of tuple\r\n6-1 | (Sid & Lei) Performance evaluation\r\n    | (Sid & Lei) Non-SIMD vs SIMD sort version in peloton\r\n6-2 | Prepare for the document and poster\r\n## Work Completed\r\n\r\n### Sequential Merge-Sort Join\r\nIn Peloton's plan tree, a merge-sort join could be represented as a merge-sort-join node. Each child node of it is a Merge-sort operator, which returns one tier at each call. The merge-sort join operator call the sorting operator of both sides accordingly and generate the final result.\r\n\r\n### Sorting Network\r\n\r\n<center>\r\n    <img src=\"images/sort64.png\">\r\n</center>\r\n\r\n\r\n\r\n### Bitonic Sort\r\n<center>\r\n    <img src=\"images/figure1.png\">\r\n</center>\r\n\r\n<center>\r\n    <img src=\"images/figure2.png\">\r\n</center>\r\nThe sorting network gives us runs of 8 sorted integers. The next phase is to iteratively merge these sorted runs to produce a globally sorted array of the input elements. THis is achieved using 2 levels of merge. The lowest level (that is expected to entirely fit in L1 cache) is bitonic merge. This phase uses AVX2 intrinsics to vectorize the process of mergeing 2 sorted registers. The algorithm contains the following steps (summarized in figure 1)\r\n\r\n1. Consider 2 sorted input registers A and B. Reverse B.\r\n2. Perform min-max with the corresponding elements of each register - the first element of A is compared against the first element of reversed B and so on. This is termed as \"Interleaved Minmax\" and performs a comparison for all 8 elements. For this reason, this step can be directly implemented using AVX2 min-max instructions.\r\n3. After step 2, the bitonic merge algorithm guarantees that all the first 8 elements are smaller than the next 8 elements. The next step is performed separately for each input register. Within each register, the first element is compared against the 5th, second to 6th and so on. 4 comparisons are performed to produce output runs where the first 4 elements are smaller than the next 4 elements. This process of halving the problem size is continued till all elements are sorted.\r\n4. Step 3 minmax cannot be direct performed using AVX2 as it does not involve two registers. Instead an \"unrolling\" approach (as shown in Figure 2) is followed by creating a permutation of the given register and eventually completing step 3/\r\n\r\nFigure 2 is annotated with the latency of each avx2 instruction used to implement intra-register bitonic merge\r\n\r\nAt the end of bitonic merge we receive two registers with globally sorted elements.\r\n\r\n### Generalized Merge\r\nFrom the previous phase, bitonic merge can be directly applied to obtain runs of 16 elements that are sorted. Further levels of merge build up on 8-by-8 bitonic merge to completely sort the array. For instance, to merge 2 sorted 16 elements arrays, we perform the following steps -\r\n\r\n1.  Apply Bitonic merge for the first 8 elements of the two input arrays. This outputs 2 registers where the first register (A) has sorted elements that are all smaller than the elements of the second sorted register (B). We store the contents of A into the output buffer. We then load in the next elements from the array which has the smaller element at the current pointer position and compare it against B.\r\n2. The process described in (1) is applied by consuming 8 elements at a time from either array to finally produce a sorted array of 16 elements.\r\n3. If there are n input elements, generalized merge repeats step 1 and 2 O(log n) times to completely sort the input array.\r\n\r\n## Process\r\nThe process goes as the plan in proposal. By the end of week three, we have finished the minimum goal: implementation of the logistic of the SIMD version merge-sort. We are now focusing on expanding the functionality and conducting performance tests for our SIMD merge-sort. After that, we will deploy the merge sort in Peloton and reach our major goal. \r\n\r\n\r\n## Poster Session Plan\r\nWe can show a simple demo by performing our merge-sort in peloton. In the meantime, we will use graph and charts to show the performance of our implementation.\r\n\r\n## Issues\r\n* Peloton uses tuple to store data, we need to figure out how to maintain the reference of tuple while doing sort.\r\n* Currently our methods only support array lengths which are times of 64\r\n\r\n## Authors and Contributors\r\nThe project is planed to be done evenly by Siddharth Santurkar (@sid1607) and Lei Qi (@xhad1234).\r\n\r\n\r\n***\r\n\r\n***\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}